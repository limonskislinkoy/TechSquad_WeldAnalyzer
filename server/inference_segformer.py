import torch
import numpy as np
import cv2
from PIL import Image
from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation
import os
from typing import Tuple

# ---------------------------
# ðŸ”§ ÐšÐ¾Ð½Ñ„Ð¸Ð³ÑƒÑ€Ð°Ñ†Ð¸Ñ
# ---------------------------
BASE_MODEL = "nvidia/segformer-b4-finetuned-ade-512-512"  # Ð±Ð°Ð·Ð¾Ð²Ð°Ñ Ð°Ñ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°
WEIGHTS_PATH = "segformer_output/segformer_best_checkpoint.pth"  # Ð¿ÑƒÑ‚ÑŒ Ðº .pth Ñ„Ð°Ð¹Ð»Ñƒ
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

NUM_CLASSES = 14  # âš ï¸ Ð—Ð°Ð¼ÐµÐ½Ð¸Ñ‚Ðµ Ð½Ð° Ð½ÑƒÐ¶Ð½Ð¾Ðµ Ð²Ð°Ð¼ Ñ‡Ð¸ÑÐ»Ð¾ ÐºÐ»Ð°ÑÑÐ¾Ð²
TILE_SIZE = 512
OVERLAP_X = 100
OFFSET_Y_TOP = 200
OFFSET_Y_BOTTOM = 440
CLAHE_CLIP = 3.0
CLAHE_GRID = (8, 8)

SHARPEN_KERNEL = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])

# ---------------------------
# ðŸ§  Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸
# ---------------------------
print("Ð˜Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¼Ð¾Ð´ÐµÐ»Ð¸...")
feature_extractor = SegformerFeatureExtractor.from_pretrained(BASE_MODEL)

# âš ï¸ id2label Ð¸ label2id Ð¼Ð¾Ð¶Ð½Ð¾ Ð¿Ð¾Ð´ÑÑ‚Ð°Ð²Ð¸Ñ‚ÑŒ Ð¸Ð· Ð²Ð°ÑˆÐµÐ¹ Ð·Ð°Ð´Ð°Ñ‡Ð¸
id2label = {i: f"class_{i}" for i in range(NUM_CLASSES)}
label2id = {v: k for k, v in id2label.items()}

model = SegformerForSemanticSegmentation.from_pretrained(
    BASE_MODEL,
    num_labels=NUM_CLASSES,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)

print(f"Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð²ÐµÑÐ¾Ð² Ð¸Ð· {WEIGHTS_PATH}...")
checkpoint = torch.load(WEIGHTS_PATH, map_location=DEVICE, weights_only=False)
model.load_state_dict(checkpoint["model_state_dict"])
model.to(DEVICE)
model.eval()


# ---------------------------
# ðŸ–¼ï¸ ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¸Ð·Ð¾Ð±Ñ€Ð°Ð¶ÐµÐ½Ð¸Ñ
# ---------------------------
def prepare_input_image(pil_img: Image.Image) -> Image.Image:
    gray = np.array(pil_img.convert("L"))
    clahe = cv2.createCLAHE(clipLimit=CLAHE_CLIP, tileGridSize=CLAHE_GRID)
    clahe_gray = clahe.apply(gray)
    sharpened_gray = cv2.filter2D(gray, -1, SHARPEN_KERNEL)
    merged = cv2.merge([gray, clahe_gray, sharpened_gray])
    return Image.fromarray(merged)


# ---------------------------
# ðŸ”ª ÐÐ°Ñ€ÐµÐ·ÐºÐ° Ñ‚Ð°Ð¹Ð»Ð¾Ð²
# ---------------------------
def generate_tiles(img: Image.Image) -> Tuple[list, list]:
    width, height = img.size
    tiles, positions = [], []
    for y0 in [OFFSET_Y_TOP, OFFSET_Y_BOTTOM]:
        y1 = y0 + TILE_SIZE
        if y1 > height:
            continue
        x = 0
        while x + TILE_SIZE <= width:
            box = (x, y0, x + TILE_SIZE, y1)
            tile = img.crop(box)
            tiles.append(tile)
            positions.append((x, y0))
            x += TILE_SIZE - OVERLAP_X
    return tiles, positions


# ---------------------------
# ðŸ¤– ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð¾Ð´Ð½Ð¾Ð³Ð¾ Ñ‚Ð°Ð¹Ð»Ð°
# ---------------------------
def predict_tile(tile: Image.Image) -> np.ndarray:
    tile_rgb = prepare_input_image(tile)
    encoding = feature_extractor(images=tile_rgb, return_tensors="pt")
    pixel_values = encoding["pixel_values"].to(DEVICE)

    with torch.no_grad():
        outputs = model(pixel_values)
        logits = outputs.logits
        logits = torch.nn.functional.interpolate(
            logits, size=(TILE_SIZE, TILE_SIZE), mode="bilinear", align_corners=False
        )
        pred_mask = torch.argmax(logits, dim=1).squeeze().cpu().numpy()

    return pred_mask


# ---------------------------
# ðŸ§© Ð¡Ð±Ð¾Ñ€ÐºÐ° Ñ„Ð¸Ð½Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ð°ÑÐºÐ¸
# ---------------------------
def reconstruct_mask(tiles_preds, positions, img_size):
    full_mask = np.zeros(img_size[::-1], dtype=np.int32)
    count_mask = np.zeros_like(full_mask, dtype=np.int32)

    for tile, (x, y) in zip(tiles_preds, positions):
        full_mask[y : y + TILE_SIZE, x : x + TILE_SIZE] += tile
        count_mask[y : y + TILE_SIZE, x : x + TILE_SIZE] += 1

    count_mask[count_mask == 0] = 1
    averaged_mask = full_mask / count_mask
    return averaged_mask.astype(np.uint8)


# ---------------------------
# ðŸš€ Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ñ
# ---------------------------
def segment_large_image(image_path: str) -> np.ndarray:
    original_image = Image.open(image_path).convert("RGB")
    tiles, positions = generate_tiles(original_image)

    print(f"Processing {len(tiles)} tiles...")

    predictions = [predict_tile(tile) for tile in tiles]
    final_mask = reconstruct_mask(predictions, positions, original_image.size)
    return final_mask


# ---------------------------
# ðŸ“¦ Ð¢Ð¾Ñ‡ÐºÐ° Ð²Ñ…Ð¾Ð´Ð°
# ---------------------------
if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage: python segment_script.py path_to_image.jpg")
    else:
        input_path = sys.argv[1]
        result_mask = segment_large_image(input_path)
        output_path = os.path.splitext(input_path)[0] + "_mask.png"
        cv2.imwrite(output_path, result_mask)
        print(f"Saved segmentation mask to {output_path}")
